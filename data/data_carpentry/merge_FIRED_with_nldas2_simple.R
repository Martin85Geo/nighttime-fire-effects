# Purpose: merge the CONUS FIRED database of MODIS burned area-derived fire event perimeters with the
# hourly climate data extracted from NLDAS2 representing the time series of climate through the
# course of each event (+3 days before ignition and +3 days after extinction)

library(tidyverse) # for piping and dplyr data manipulation
library(sf) # for spatial vector data
library(data.table) # for fast reading and manipulation of giant dataframes
library(lubridate) # for tools to more easily work with datetimes

# This function will return NLDAS2 climate data extracted for FIRED events in the given year
# It will first check whether these files are available locally and will read them from disk
# if so. If they aren't available locally, they will be downloaded from the public files on
# the S3 bucket. If download=TRUE (the default), then the files will be written to disk 
# locally to speed up the process of reading them for next time.
get_FIREDnldas2 <- function(year, download = TRUE) {
  
  if(file.exists(paste0("data/data_output/FIRED-with-nldas2-climate-variables_CONUS/FIRED-nldas2-climate-variables_CONUS_", year, ".csv"))) {
    # If the file exists locally, just read it
    (FIRED_climateVars_thisYear <- 
       data.table::fread(paste0("data/data_output/FIRED-with-nldas2-climate-variables_CONUS/FIRED-nldas2-climate-variables_CONUS_", year, ".csv")))
    
  } else {
    # otherwise try to read it from the S3 bucket; if it isn't uploaded to the S3 bucket yet, a try-error will
    # be returned and the function will return NULL but still continue to the next year to try
    (FIRED_climateVars_thisYear <- try(data.table::fread(paste0("https://earthlab-mkoontz.s3-us-west-2.amazonaws.com/FIRED-with-nldas2-climate-variables_CONUS/FIRED-nldas2-climate-variables_CONUS_", year, ".csv"))))
    # check if the read from the S3 bucket was successful
    if ("try-error" %in% class(FIRED_climateVars_thisYear)) {
      # If not successful, the object that will eventually be returned by the function is assigned to be NULL
      FIRED_climateVars_thisYear <- NULL
      
    } else {
      # If the read from S3 was successful and download=TRUE (the default), then the file will be written to
      # the data/data_output/ folder, which will be created if it doesn't exist yet
      if (download) {
        if (!dir.exists("data/data_output/FIRED-with-nldas2-climate-variables_CONUS")) {
          dir.create("data/data_output/FIRED-with-nldas2-climate-variables_CONUS")
        }
        data.table::fwrite(x = FIRED_climateVars_thisYear, 
                           file = paste0("data/data_output/FIRED-with-nldas2-climate-variables_CONUS/FIRED-nldas2-climate-variables_CONUS_", year, ".csv"))
      }
    }
  }
  # Print the year of the data that was just extracted
  print(year)
  
  return(FIRED_climateVars_thisYear)
}

# Iterate through all years and apply the function to get that year's data
FIRED <- lapply(2000:2019, get_FIREDnldas2)

# Filter out the list elements for which the data could not be read (i.e., NULL was returned
# because the file wasn't available locally or on the S3 bucket)
# Then bind all the dataframes together
FIRED_filtered <- 
  FIRED[which(purrr::map_lgl(FIRED, .f = function(x) {return(!is.null(x))}))] %>% 
  data.table::rbindlist() 

# Assign the datetime of each NLDAS2 image by converting the column that represents
# the number of milliseconds since 1970-01-01 00:00Z
FIRED_filtered[, datetime := as.POSIXct(nldas_datetime / 1000,
                                        origin = "1970-01-01", 
                                        tz = "zulu")]

# Break down the time components of the datetime
FIRED_filtered <-
  FIRED_filtered[, `:=`(year = year(datetime),
                        month = month(datetime),
                        day = day(datetime),
                        hour = hour(datetime),
                        minute = minute(datetime),
                        .geo = NULL)] %>% 
  # separate the system index into pieces by separating on the underscore
  # Pieces represent the FIRED system index (generated by Earth Engine-- unique per event),
  # the day index of the NLDAS2 image, and the timestamp index of the NLDAS2 product
  tidyr::separate(col = 'system:index', into = c("ee.FIRED.system.index",
                                                 "NLDAS2.system.index",
                                                 "NLDAS2.timestamp")) %>% 
  # Reunite the NLDAS2 indices
  tidyr::unite(col = ee.NLDAS2.system.index, NLDAS2.system.index, NLDAS2.timestamp)

# This table will link each unique EE-generated index for the FIRED database to the original
# unique identifier for each feature in the FIRED database
ee_system.index_to_FIRED_id_conversion_table <- data.table::fread("https://earthlab-mkoontz.s3-us-west-2.amazonaws.com/FIRED-with-nldas2-climate-variables_CONUS/ee-FIRED-system.index_to_FIRED-id_conversion.csv")

# This is the raw CONUS FIRED database (just copied over from the earthlab-natem S3 bucket)
# It contains all the important information from the original FIRED database including the geometry
FIRED_meta_spatial <- st_read("https://earthlab-mkoontz.s3-us-west-2.amazonaws.com/FIRED-with-nldas2-climate-variables_CONUS/modis_event_polygons_cus.gpkg")

# Join the two metadata pieces so we can link the Earth Engine index with the rest of the 
# original FIRED metadata
FIRED_meta <- dplyr::left_join(FIRED_meta_spatial, ee_system.index_to_FIRED_id_conversion_table)

# Combine all the NLDAS2 observations per FIRED event with the metadata from each fired event
# Might be an unnecessary step at this point, because working with the geometry column intact
# adds some processing time. Probably best to use the FIRED_meta object generated above to aggregate
# the active fire detections within each FIRED event, then drop the geometry, then proceed with
# merging.
FIRED_nldas <- dplyr::left_join(FIRED_meta, FIRED_filtered, by = "ee.FIRED.system.index")
